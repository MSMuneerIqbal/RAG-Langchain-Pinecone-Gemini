# -*- coding: utf-8 -*-
"""RAG-Langchain-Pinecone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/148bZKVW-QoJ_21BX41KUtlX6NP9p5CQq
"""

!pip install -qU python-dotenv langchain pinecone-client google-generativeai tqdm

import os
from dotenv import load_dotenv
from google.colab import userdata
# retrieve and set the environment variable
os.environ['PINECONE_API_KEY'] = userdata.get('PINECONE_API_KEY')
os.environ['PINECONE_ENVIRONMENT'] = userdata.get('PINECONE_ENVIRONMENT_KEY')
#os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')
os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY2')

from pinecone import Pinecone
pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'], environment = os.environ['PINECONE_ENVIRONMENT'])

# connecting to an existing index
index_name = 'rag-1'
index = pc.Index(index_name)
print(f'Successfully connected to index: {index}')

!pip install -qU langchain-google-genai
from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings
embeddings = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",
    google_api_key=os.environ['GOOGLE_API_KEY']
)

!pip install -qU langchain-community # Corrected the package name from lanchain-community to langchain-community

!pip install -qU pypdf
# from langchain.document_loaders import PDFLoader  # This line is replaced
from langchain.document_loaders import PyPDFLoader    # Import PyPDFLoader instead
from langchain.text_splitter import RecursiveCharacterTextSplitter

# load document
loader = PyPDFLoader("/content/Effective Software.pdf")
documents = loader.load()

# split documents in to chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = text_splitter.split_documents(documents)

total_text = "".join([doc.page_content for doc in documents])
total_characters = len(total_text)
print(f"Total characters: {total_characters}")

print(f"Number of chunks: {len(docs)}")

from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings

# Ensure the embeddings variable is assigned correctly
embeddings = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",
    google_api_key=os.environ['GOOGLE_API_KEY']
)

from tqdm import tqdm

# create embeddings and upload to pinecone
for doc in tqdm(docs):
  # Now you should be able to call embed_query on the embeddings object
  vector = embeddings.embed_query(doc.page_content)

print(vector[:10])

# create embeddings and upload to pinecone
from tqdm import tqdm
for i, doc in enumerate(tqdm(docs)):
  # generate embeddings for the current document
  vector = embeddings.embed_query(doc.page_content)
  # upsert with the correct format (id, vector, metadata)
  # Include the document content as metadata under the key "text"
  index.upsert(vectors=[(str(i), vector, {"text": doc.page_content})])

from langchain.vectorstores import Pinecone
retriever = Pinecone.from_existing_index(index_name=index_name, embedding=embeddings, text_key="text")

! pip install -qU langchain-google-genai

from langchain_google_genai import ChatGoogleGenerativeAI
gemini_model = ChatGoogleGenerativeAI(api_key=os.environ['GOOGLE_API_KEY'],model="gemini-1.5-flash",temperature=0.7)

from langchain.chains import RetrievalQA
from langchain.vectorstores.base import VectorStoreRetriever

# create vectore store retriever from your pinecone index
retriever = VectorStoreRetriever(vectorstore=Pinecone.from_existing_index(index_name=index_name, embedding=embeddings, text_key="text"))

# create a chain to answer questions
qa_chain = RetrievalQA.from_chain_type(llm=gemini_model, chain_type="map_reduce", retriever=retriever)

query = "give details Effective Software Engineer"
response = qa_chain.run(query)  # Use invoke() instead of run()
print(f"User Query: {query}")
print()
print(f"AI Response: {response}")

query = "tell about Time Management"
response = qa_chain.run(query)  # Use invoke() instead of run()
print(f"User Query: {query}")
print()
print(f"AI Response: {response}")

query = "how many qualities of effective software engineer"
response = qa_chain.run(query)  # Use invoke() instead of run()
print(f"User Query: {query}")
print()
print(f"AI Response: {response}")